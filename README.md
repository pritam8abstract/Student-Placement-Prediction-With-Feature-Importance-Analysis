# ğŸ“ Student Placement Prediction: A Machine Learning Approach

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![Pandas](https://img.shields.io/badge/Pandas-1.3+-green.svg)](https://pandas.pydata.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **Predicting student placement outcomes using advanced machine learning techniques to enable targeted interventions and career guidance.**

## ğŸš€ Project Overview

This project tackles the critical challenge of predicting student placement outcomes using machine learning, based on the Kaggle challenge "Placement Puzzle: Crack the Hiring Code." The system analyzes ~300 features encompassing academic performance, skills assessment, and background information to predict placement success.

### ğŸ¯ Key Achievements
- **80% Overall Accuracy** with Random Forest model
- **Comprehensive Feature Analysis** using 4 different importance techniques
- **Multi-Model Comparison** across 4 state-of-the-art algorithms
- **Production-Ready Pipeline** with automated preprocessing

## ğŸ“Š Dataset & Features

### Data Structure
- **Training Set**: ~300 features across multiple dimensions
- **Target Variable**: Binary classification (0 = Placed, 1 = Not Placed)
- **Feature Categories**:
  - ğŸ“š **Academic Performance**: Board types, percentage scores (SSC, HSC, MBA)
  - ğŸ¯ **Entrance Examinations**: Test scores and percentiles
  - ğŸ’¬ **Skills Assessment**: Communication, project work, specialized tests
  - ğŸ‘¤ **Demographics**: Gender, work experience, specialization
  - ğŸ« **Educational Background**: Stream, specialization details

### Data Preprocessing Pipeline
```python
# Automated preprocessing with sklearn Pipeline
preprocessor = ColumnTransformer([
    ('num', numerical_transformer, numerical_cols),
    ('cat', categorical_transformer, categorical_cols)
])
```

**Key Preprocessing Steps:**
- **Missing Value Imputation**: Statistical imputation for 53 null values in critical features
- **Feature Scaling**: StandardScaler for numerical features
- **Categorical Encoding**: One-hot encoding with unknown category handling
- **Automated Type Detection**: Dynamic identification of feature types

## ğŸ¤– Machine Learning Models

### Model Performance Comparison

| Model | Overall Accuracy | F1 Score (Not Placed) | Best Use Case |
|-------|-----------------|----------------------|---------------|
| **Random Forest** | **80.00%** | 0.0000 | High accuracy prediction |
| **XGBoost** | 76.67% | 0.1250 | Balanced performance |
| **Logistic Regression** | 75.00% | **0.2105** | Minority class detection |
| **Gradient Boosting** | 73.33% | 0.1111 | Sequential learning |

### Model Architectures

#### ğŸŒ² Random Forest
- **Ensemble Method**: Bootstrap aggregation with multiple decision trees
- **Advantages**: Robust to overfitting, handles mixed data types
- **Implementation**: Optimized for high-dimensional feature space

#### ğŸš€ XGBoost
- **Gradient Boosting**: Advanced optimization with regularization
- **Features**: Built-in missing value handling, parallel processing
- **Performance**: Superior computational efficiency

#### ğŸ“ˆ Logistic Regression
- **Linear Model**: Probabilistic interpretation with feature coefficients
- **Strength**: Best minority class detection (F1: 0.2105)
- **Interpretability**: Clear feature importance through coefficients

#### ğŸ”„ Gradient Boosting
- **Sequential Learning**: Iterative error correction approach
- **Focus**: Misclassified instance improvement

## ğŸ” Feature Importance Analysis

### Multi-Method Feature Ranking

Applied **4 complementary techniques** for comprehensive feature analysis:

1. **ğŸ“Š Correlation Analysis**
   ```python
   placement_correlations = X.apply(lambda col: col.corr(y))
   ```

2. **ğŸ§  Mutual Information Score**
   ```python
   mi_scores = mutual_info_classif(X, y)
   ```

3. **ğŸ“‹ Chi-Square Testing**
   ```python
   chi2_selector = SelectKBest(chi2, k=10)
   ```

4. **âš–ï¸ Logistic Regression Coefficients**
   ```python
   coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': np.abs(lr.coef_[0])})
   ```

### ğŸ† Most Important Features

**Consistently High-Impact Predictors:**
- ğŸ“ **Board Examination Types** (Board_HSC, Board_SSC)
- ğŸ“ **Entrance Test Performance** (Entrance_Test, Percentile_ET)
- ğŸ’¬ **Communication Skills** (Marks_Communication)
- ğŸ’¼ **Work Experience** (Experience_Yrs)
- ğŸ¯ **Specialized Assessments** (S-TEST, S-TEST*SCORE)
- ğŸ“Š **Academic Percentages** (Percent_SSC, Percent_HSC, Percent_MBA)

## ğŸ“ˆ Results & Insights

### Performance Analysis
- **High Overall Accuracy**: Models achieve 73-80% accuracy
- **Class Imbalance Challenge**: Low F1 scores for "Not Placed" class indicate difficulty in minority class prediction
- **Model Trade-offs**: Accuracy vs. minority class detection creates interesting optimization challenges

### Key Findings
1. **Academic Foundation Matters**: Board examination types significantly impact outcomes
2. **Communication is Critical**: Soft skills (communication) rank among top predictors
3. **Experience Advantage**: Prior work experience strongly correlates with placement success
4. **Standardized Tests**: Entrance exam performance remains a strong predictor

## ğŸ› ï¸ Technical Implementation

### Project Structure
```
placement-prediction/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Train_Features.csv
â”‚   â”œâ”€â”€ Train_Target.csv
â”‚   â””â”€â”€ Test_Features.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ student_placement_model.py
â”‚   â”œâ”€â”€ feature_importance_analysis.py
â”‚   â””â”€â”€ preprocessing.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ confusion_matrices/
â”‚   â”œâ”€â”€ feature_importance_plots/
â”‚   â””â”€â”€ final_submission.csv
â””â”€â”€ README.md
```

### Key Technologies
- **ğŸ Python 3.8+**: Core programming language
- **ğŸ”¬ scikit-learn**: Machine learning framework
- **ğŸ¼ Pandas**: Data manipulation and analysis
- **ğŸ“Š Matplotlib/Seaborn**: Visualization and plotting
- **âš¡ XGBoost**: Advanced gradient boosting

### Model Pipeline
```python
# Complete ML Pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', model)
])

# Cross-validation and evaluation
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
```

## ğŸ¯ Future Improvements

### Planned Enhancements
- **ğŸ”„ SMOTE Implementation**: Address class imbalance with synthetic oversampling
- **ğŸ›ï¸ Hyperparameter Tuning**: Grid search optimization for all models
- **ğŸ§ª Ensemble Methods**: Custom voting/stacking classifiers
- **ğŸ§  Deep Learning**: Neural network exploration for complex pattern recognition
- **ğŸ“Š Feature Engineering**: Interaction terms and polynomial features

## ğŸ“š Educational Impact

### Practical Applications
- **ğŸ¯ Early Warning System**: Identify at-risk students for targeted support
- **ğŸ“‹ Curriculum Development**: Inform program improvements based on key predictors
- **ğŸ’¼ Career Guidance**: Data-driven career counseling recommendations
- **ğŸ« Resource Allocation**: Optimize support service distribution

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/yourusername/student-placement-prediction.git
cd student-placement-prediction
pip install -r requirements.txt
```

### Usage
```python
from src.student_placement_model import PlacementPredictor

# Initialize and train model
predictor = PlacementPredictor()
predictor.load_data('data/')
predictor.train_models()

# Make predictions
predictions = predictor.predict(test_data)
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## ğŸ“ Contact

**Dhritabrata Swarnakar**
- Email: pritamswarnakar21@gmail.com
- GitHub: [@yourusername](https://github.com/yourusername)

---

â­ **Star this repository if you found it helpful!** â­
